{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6: Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Material for this worksheet can be found in the AE2220-I lecture notes Chapter 9.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Optimization consists in finding $x\\in \\Omega$ such that some function $f(x)$ is minimized (or maximized) on the domain $\\Omega$. The optimal solution $\\overline{x}$, in case of a minimization problem, is such that: \n",
    "$$f(\\overline{x})\\leq f(x),~~~~~~~~~~ \\forall x\\in \\Omega$$\n",
    "\n",
    "where $f(\\cdot) $ is called the objective function and $x$ is the vector of design variables.\n",
    "\n",
    "In a compact-normalized way a minimization problem can be written as follows: \n",
    "$$\n",
    "\\min_{x\\in\\Omega} f(x)  ~~~~~~ subject~to \n",
    "$$\n",
    "$$\n",
    "g(x)=0, ~~~ and \n",
    "$$\n",
    "$$\n",
    "h(x)\\geq 0,\n",
    "$$\n",
    "\n",
    "where g(x) is called an equality constraint and h(x) an inequality constraint.\n",
    "\n",
    "Optimization problems may have no-solution, infinite solutions and, in some special cases, one solution. Optimization algorithms have the problem that they usually converge to local optimum, and this can be an issue if we are looking for a global-best value. Another weakness of optimization is that usually algorithms are highly sensitive to the starting point: a different starting point can lead to a different solution. \n",
    "\n",
    "Optimization algorithms are divided into gradient-based methods that use $f'$ (Steepest descent, Newton's method), and gradient-free methods (Golden-section method) that require only to evaluate $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Golden-section search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Golden-section search is a general easy method since it is a gradient-free method (no derivatives need to be evaluate) and only requires one function evaluation per iteration. At each step of the Golden-section algorithm the interval length ( the intervall that is supposed to contain the global/local minimum) is multiplied by $\\frac{1}{\\varphi} $ where $\\varphi=1.6180 $ is the Golden ratio, this means that at each iteration the interval will shrink of a factor equal to the Golden ratio. \n",
    "\n",
    "The Golden method is guaranteed to converge to a minimum but generally we don't know if it is a local or global minimum.  Only if we can assume the the cost function is *unimodal* then the method will always converge to the unique global minimum.\n",
    "\n",
    "**Definition:** A *unimodal* function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ has a single minimum at $x^*$, and $f(x)$ is monotonically decreasing for $x\\leq x^*$ and monotonically increasing for $x\\geq x^*$. A convex Parabola is the most simple example of a unimodal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'axes.labelsize': 18})\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Consider the following unimodular function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\n",
    "$$\n",
    "f(x) = \\begin{cases} 3\\sqrt{3-x}+1    & x \\leq 3 \\\\ (x-3)^2 + 1    &  3 \\leq  x \\leq 5 \\\\ \\frac{3}{2}ln(x-4) +5 & x \\geq 5 \\end{cases}.\n",
    "$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construction of the function f(x) as given above\n",
    "from math import log\n",
    "def function(x):\n",
    "    if x<=3:            value = 3*(3-x)**0.5 +1\n",
    "    elif x>3 and x<=5:  value = (x-3)**2 + 1\n",
    "    elif x>5:           value = (3/2)*log(x-4)+5\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the function f(x)\n",
    "xx = np.linspace(0,10,101)\n",
    "f_x = np.zeros(len(xx))\n",
    "for i in range(101):\n",
    "    f_x[i] = function(xx[i])\n",
    "plt.plot(xx, f_x)\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$f(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Implement the golden section method and apply it to f(x) as defined above. Use [0, 10] as the\n",
    "initial interval containing the minimum. Report the value of x at which the golden method converge (Consider the convergence of the method reached when the error term between the actual iteration and the previous is less than 0.0001 ). After how many iteration the method converge?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def golden_method(a,b): # a and b are the interval values, in our case [0,10], a=0, b=10\n",
    "    pass ### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Results from the golden_method: x_op = point of convergence of the method, count= tot number of iterations \n",
    "[x_op,count] = golden_method(0,10)\n",
    "print(x_op,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)Sketch a unimodular function that is not continuous everywhere on its domain. Explain why\n",
    "the golden section method still works if the unimodular function is not continuous.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method\n",
    "\n",
    "Newton's method in optimization is the same as the already familiar Newton's method used in the root finding:\n",
    "$$ x_{n+1}= x_n - \\frac{g(x_n)}{g^\\prime(x_n)} $$\n",
    "\n",
    "The formula above is useful to find the value of $x^*$ such that $g(x^*)=0$. In optimization we are interested in finding the value of $\\overline{x}$ such that $f(\\overline{x})$ is minimized/maximized. In other words we are interested in finding $\\overline{x}$ such that $f^\\prime(\\overline{x})=0$. So if we want to relate the root finding with optimization we should have that $g(x)=f^\\prime(x)$. The Newton's formula to find the optimal x-value will be: \n",
    "$$ x_{n+1}= x_n - \\frac{f^\\prime(x_n)}{f''(x_n)} $$\n",
    "\n",
    "In this case the requirement is that f(x) is twice continuously differentiable on its domain of definition. \n",
    "\n",
    "However, assuming that the Newton formula converge to a certain value $\\overline{x}$, the following question raise:\n",
    "Is $\\overline{x}$ a local point of maximum, local minima or saddle point?\n",
    "\n",
    "To answer this question the Hessian matrix $f''(\\overline{x})$ need to be examined ( In 1D the Hessian matrix is just a scalar). If all the eigenvalues are positive we have a local minima, all negative a local maxima, and mixed implies a saddle point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this exercise we want to find a global/local minimum of f(x) by applying the Newton's method.**\n",
    "\n",
    "**Let's consider the following function f(x):**\n",
    "\n",
    "**$$f(x)= \\frac{\\cos(3\\pi x)}{x} $$**\n",
    "\n",
    "**(a) Apply Newton's method with the following starting point $x_0 =0.4 $. At which x-value does the Newton's method converge? What can we say about this point (local maxima, local minima or saddle point)?**\n",
    "\n",
    "**(b) Apply again the Newton's method but this time with a different starting point $x_0 =1.20 $. At which x-value does the Newton's method converge?**\n",
    "\n",
    "**(c) Comparing part (a) and part (b), What can you conclude about Newton's method?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construction of f(x) \n",
    "from math import log\n",
    "def f(x):   \n",
    "    return np.cos(np.pi*3*x)/x\n",
    "a, b = 0.1, 3\n",
    "\n",
    "xx = np.linspace(a,b,101)\n",
    "plt.plot(xx, f(xx))\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$f(x)$')\n",
    "\n",
    "#Construction of f'(x)\n",
    "def f_prime(x):   \n",
    "    return np.pi*-3*np.sin(np.pi*3*x)/(x) - np.cos(np.pi*3*x)/(x**2)\n",
    "\n",
    "#Construction of f''(x)\n",
    "def f_second(x):   \n",
    "    return 6*np.pi*np.sin(3*np.pi*x)/x**2 + (np.cos(3*np.pi*x)/x)*(2/x**2 -9*np.pi**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_method(x0): # x0 is the starting point\n",
    "    pass ### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Results from the newton_method: x_opt = value of convergence of Newton's method, xn = stores all the iteration , fn \n",
    "#stores all the f(xn)\n",
    "x_opt,xn,fn = newton_method(0.4)\n",
    "print(x_opt)\n",
    "\n",
    "xx = np.linspace(a,b,101)\n",
    "plt.plot(xx, f(xx))\n",
    "plt.plot(xn, fn,'ok')\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$f(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steepest descent method is an alternative to the Newton's method when we want to minimize $f(x)$ and we only have information regarding $f'(x)$. The goal of this method is to find a descent direction such that going from iteration $x^n$ to iteration $x^{(n+1)}$ we have that $f(x^{(n+1)}) \\leq f(x^{(n)})$. The direction is simply $r_n = -f'(x^{(n)})$  (in multy dimension we will have $r_n = -\\bigtriangledown f'(x^{(n)}$ ) .\n",
    "The general iteration scheme for the steepest descent method will be as follows:\n",
    "\n",
    "$$ x^{(n+1)} = x^{(n)} - \\alpha^{(n)} \\cdot \\bigtriangledown f'(x^{(n)}) $$\n",
    "\n",
    "where $\\alpha^{(n)}$ can be seen as an optimal step length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: **\n",
    "** Consider the following quadratic function f(x):**\n",
    "**$$ f(x,y)= 4x^2 − 4xy + 2y^2$$**\n",
    "\n",
    "**(a) Apply the Steepest descent method with constant $\\alpha=0.1$ and with starting point $x_0 = [2,3]$. Does the method converge? If yes, to which value and after how many iterations? **\n",
    "\n",
    "**(b) Repeat part a but this time with $\\alpha=0.2$. Does the method still converge? What can you conclude by comparing results from part a and part b?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construction of the gradient \n",
    "def grad( x ):\n",
    "    b = np.array([0, 0])\n",
    "    b[0]= 8*x[0]-4*x[1]\n",
    "    b[1]= -4*x[0] + 4*x[1] \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def steepest_descent(x0,alpha): # x0 is the starting point, alpha is the step length\n",
    "    pass ### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_0 = [2,3] #initial value\n",
    "alpha = 0.1 # step length\n",
    "(x_opt,cout) = steepest_descent(x_0,0.1)\n",
    "print(x_opt,cout)  # print out the convergence value (x_opt) and after how many iteration this value is reached"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
