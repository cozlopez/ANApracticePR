{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic Spline Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Material for this worksheet can be found in the AE2220-I lecture notes Chapter 4.1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous module we saw that polynomial interpolation can be oscillatory. In this circumstance, higher degree polynomials, and more samples do not necessarily mean lower error (see notes, pg. 31). Furthermore, we saw this is especially the case with uniformly-spaced samples - and sometimes we are not free to choose $x_i$.  This is the case in e.g. most forms of time-sampling.\n",
    "\n",
    "Once again we are concerned with approximating a smooth function of one variable $f(x)$, on the interval $[a, b]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "Ia, Ib = -1.0, 1.0                 ### Interval endpoints\n",
    "N = 4                              ### Number of sub-intervals\n",
    "xi = np.linspace(Ia,Ib,N+1)        ### Uniform sample locs  \n",
    "\n",
    "### Functions to test - and 2nd derivatives\n",
    "\n",
    "def f_sin(x): return np.sin(3*x)         ### Sine wave  \n",
    "def d2fdx2_sin(x): return -3**2*np.sin(3*x)\n",
    "\n",
    "def f_runge(x): return 1./(1+(4.*x)**2)  ### Runge fn\n",
    "def d2fdx2_runge(x): \n",
    "    fx = f_runge(x)\n",
    "    dfdx = -32*x*fx / (1+(4.*x)**2)\n",
    "    return -(64*x*dfdx + 32*fx) / (1+(4.*x)**2)\n",
    "\n",
    "def f_step(x): return x > 0.             ### Heaviside fn\n",
    "def d2fdx2_step(x): return x * 0.\n",
    "\n",
    "def f_kink(x): return np.abs(x)          ### Kink fn\n",
    "def d2fdx2_kink(x): return x * 0.\n",
    "\n",
    "f = f_runge\n",
    "d2fdx2 = d2fdx2_runge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fi = f(xi)                         ### Sample values \n",
    "\n",
    "### Plotting\n",
    "xx = np.linspace(Ia,Ib,101)\n",
    "plt.plot(xx, f(xx))\n",
    "plt.plot(xi, f(xi), 'ok')\n",
    "plt.plot(xx, d2fdx2(xx))\n",
    "\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$f$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cubic-spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of interpolating all the samples with a single, high-degree polynomial, we can also use a collection of low-degree polynomials on <i>sub-intervals</i> of $[a,b]$. The original interval is divided into non-overlapping sub-intervals and a different polynomial fit can be used on each sub-interval. A linear polynomial on each interval would make the interpolation globally continuous, but non-differentiable at the samples.  By using higher-order polynomials we can enforce continuous derivatives as extra conditions. These collections of polynomials are called 'splines' (see notes, pg. 40)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:\n",
    "(a)  Speculate on some advanges and disadvantages of the above strategy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular choices is to use cubic polynomials, i.e. <i>cubic splines</i>:\n",
    "$$\n",
    "S_j(x) := a_j + b_j (x-x_j) + c_j (x-x_j)^2 + d_j (x-x_j)^3 \\quad\\mbox{for}\\quad x\\in[x_i,x_{i+1}] \\quad\\mbox{for}\\quad j\\in\\{0,\\dots, N-1\\}\n",
    "$$\n",
    "where $j$ indicates the interval. We have 4 coefficients on each interval, and for convenience $x$ has been offset by the start of each interval $(x - x_j)$.\n",
    "\n",
    "This choice of $S_j(x)$ has the very nice property that for $x = x_j$:\n",
    "$$\n",
    "\\begin{align}\n",
    "S_j(x_j) &= a_j \\\\\n",
    "S'_j(x_j) &= b_j \\\\\n",
    "S''_j(x_j) &= 2c_j \\\\\n",
    "S'''_j(x_j) &= 6d_j\n",
    "\\end{align}\n",
    "$$\n",
    "so that only one coefficient is involved in the derivative of any order.  This will be useful in the boundary-conditions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def S(x, xj, a, b, c, d):\n",
    "    return a + b * (x-xj) + c * (x-xj)**2 + d * (x-xj)**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) What is the maximum number of roots of a cubic?  The maximum number of minima and maxima?  Will it be possible for the spline to oscillate as did higher-order polynomials?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where now the coefficients $a$ to $d$ are defined <i>per sub-interval</i>. For splines we require, not only that the spline passes through $(x_i, f(x_i))$ (<b>interpolation</b>), but also that the 1st and 2nd derivatives are continuous at the interior nodes (<b>smoothness</b>):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_j(x_{j+1}) &= S_{j+1}(x_{j+1}) = f(x_{j+1})&\\quad\\mbox{for}\\quad &0 \\leq j \\leq N-2\\\\\n",
    "S'_j(x_{j+1}) &= S'_{j+1}(x_{j+1}) &\\quad\\mbox{for}\\quad &0 \\leq j \\leq N-2 \\\\\n",
    "S''_j(x_{j+1}) &= S''_{j+1}(x_{j+1})&\\quad\\mbox{for}\\quad &0 \\leq j \\leq N-2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "At the boundaries, the interpolation conditions become:\n",
    "$$\n",
    "\\begin{align}\n",
    "S_0(x_0) &= f(x_0) \\\\\n",
    "S_{N-1}(x_N) &= f(x_N)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This gives a total of $4(N-1) + 2$ conditions for $4N$ unknown coefficients.  So we need two more <i>boundary-conditions (BCs)</i>. The way we define these boundary conditions will define different types of cubic splines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Consider the case of enforcing smoothness of <i>third</i> derivatives.  How many unknowns are required to satisfy all conditions?  Would quartic polynomials on each interval be sufficient?  How manys BCs would be required?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Boundary Conditions and Known BCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get so-called \"natural\" BCs, we set 2nd-derivatives of the spline to $0$ at the boundary.  In particular to set the 2nd-derivative at the boundary to $0$, we only need to set $c_0 = c_N = 0$.  On the other hand, if the 2nd-derivative at the boundary is known $f''(x_0) = F$, then this can be enforced by setting $c_0 = \\frac{1}{2}F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving for coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture notes we see how these conditions can be transformed into a linear system of equations for $c_i$, from which $a$, $b$ and $d$ can be explicitly calculated.  Here $h_j$ is the width of interval $j$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = xi[1:] - xi[:-1]       # width of interval i - vector size N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear system for the $c$-coefficients can be expressed in matrix form and has left-hand side (LHS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def system_matrix(grid):\n",
    "    N = len(grid)-1\n",
    "    h = grid[1:] - grid[:-1]   # width of interval i - vector size N\n",
    "    A = np.zeros((N+1,N+1))\n",
    "    ### Equations describing conditions at interior nodes\n",
    "    for i in range(1, N):\n",
    "        A[i,i]   = 2*(h[i-1]+h[i])\n",
    "        A[i,i-1] = h[i-1]\n",
    "        A[i,i+1] = h[i]\n",
    "    return A\n",
    "A = system_matrix(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Implement the natural spline boundary conditions in A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A[?,?] = ?   ### TODO - boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) The matrix has a simple tri-diagonal structure - the vast majortity of the entries are zero, and this makes it much easier to solve for large $N$ than the Vandermonde matrix. Verify this by typing plt.imshow(A, interpolation='none')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right-hand side (RHS) of the system for $c$ can be viewed as an approximation of 2nd-derivatives of $f(x)$ (we will see this in the next module on numerical differentiation).  This makes sense since $c$ control the 2nd-derivatives of the spline at the nodes (see equation of cucbic spline above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rhs(grid, fi):           ### RHS - interior points\n",
    "    N = len(grid)-1\n",
    "    h = grid[1:] - grid[:-1]   # width of interval i - vector size N\n",
    "    rhs = np.zeros(N+1)\n",
    "    for i in range(1, N):\n",
    "        rhs[i] = 3.*(fi[i+1] - fi[i])/h[i] - 3.*(fi[i] - fi[i-1])/h[i-1]\n",
    "    return rhs\n",
    "frhs = rhs(xi, fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Natural boundary conditions\n",
    "#frhs[0] = ???    ###TODO\n",
    "#frhs[-1] = ???   ###TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are better methods to solve tridiagonal systems (an efficient form of Gaussian elimination is possible), but here we just use brute force since $N$ is small enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solve system\n",
    "c = np.linalg.solve(A, frhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute remaining coeffs\n",
    "a = fi[:]                                                  # N+1\n",
    "b = (a[1:] - a[:-1]) / h[:] - h[:]/3. * (2*c[:-1] + c[1:]) # N\n",
    "d = (c[1:] - c[:-1]) / (3. * h[:])                         # N\n",
    "\n",
    "print('a = ',a)\n",
    "print('b = ',b)\n",
    "print('c = ',c)\n",
    "print('d = ',d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "\n",
    "**(a) What happens to splines when the number of samples $N$ increses? How is their stability? [Use the function below, which performs spline interpolation, starting from samples, to function reconstruction.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spline_natural(xi, fi, xx):\n",
    "    \"\"\"\n",
    "    One-shot function for spline interpolation (with natural BCs).\n",
    "    \n",
    "    Args:\n",
    "      xi (array, n+1): Sample locations\n",
    "      fi (array, n+1): Sample values\n",
    "      xx (array, M):   Reconstuction locations\n",
    "    Return:\n",
    "      ff (array, M): Reconstructed values at xx\n",
    "    \"\"\"\n",
    "    h = xi[1:] - xi[:-1]       # Interval width\n",
    "    N = len(h)\n",
    "                               ### Setup system\n",
    "    A = system_matrix(xi)      # Left-hand side \n",
    "    frhs = rhs(xi, fi)         # Right-hand side\n",
    "    A[0,0] = A[N,N] = 1        # BC for LHS (natural)\n",
    "    frhs[0] = 0                # BC for RHS (natural)\n",
    "    frhs[-1] = 0 \n",
    "                               ### Solve system for coefficients\n",
    "    c = np.linalg.solve(A, frhs)\n",
    "    a = fi[:]                                                  # N+1\n",
    "    b = (a[1:] - a[:-1]) / h[:] - h[:]/3. * (2*c[:-1] + c[1:]) # N\n",
    "    d = (c[1:] - c[:-1]) / (3. * h[:])                         # N\n",
    "                               ### Reconstuct spline at locations xx  \n",
    "    ii = np.digitize(xx, xi)   # Find to which interval each xx belongs\n",
    "    ii = np.fmin(np.fmax(ii-1,0),N-1) \n",
    "    ff = np.zeros(xx.shape)\n",
    "    for j, i in enumerate(ii): # Compute spline for each x\n",
    "        ff[j] = S(xx[j], xi[i], a[i], b[i], c[i], d[i])\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### TODO - Question (a) - Change N and see how splines behave.\n",
    "N = 4                              ### Number of sub-intervals\n",
    "xi = np.linspace(Ia,Ib,N+1)        ### Uniform sample locs  \n",
    "fi = f(xi)                         ### Sample values \n",
    "\n",
    "spline_natural(xi, fi, xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) How well do splines perform with randomly located samples? Compare this also with polynomial interpolation. [Hint: Generate $N-1$ random numbers in $[0,1]$ with np.random.random(N-1) - and remember to sort them!]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TODO - Question (b) - Substitute uniform samples with randomly located samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)  Modify f(x) to be a function with discontinuities, or discontinuous 1st- or 2nd-derivatives. How does the spline compare to polynomial approximation in these cases? Plot the spline, the exact function and the polynomial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TODO - Question (c) - See how splines behave with discontinuous functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a spline $s(x)$ define the error\n",
    "$$\n",
    "\\epsilon_N := \\| f - s \\|_\\infty := \\max_{x\\in[a,b]} |  f(x) - s(x) |,\n",
    "$$\n",
    "i.e. the maximum distance between the spline and the exact function. From theory a spline with natural BCs should have an error which converges like $\\epsilon_N \\sim \\mathcal{O}(\\frac{1}{N^2})$, for $f\\in C^4([a,b])$. Again from theory: if the 2nd-derivatives of $f$ at the endpoints are known, and these are used as BCs instead of the natural BCs, then the error behaves like $\\epsilon_N \\sim \\mathcal{O}(\\frac{1}{N^4})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "\n",
    "**(a) Verify, with a sufficiently smooth test function $f$, that the error of a natural spline converges like $\\epsilon_N \\sim \\mathcal{O}(\\frac{1}{N^2})$.  [Note: You'll need to implement an approximation of $\\epsilon_N$, see for example the notebook for Module 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TODO (exercsie 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)  Replace the natural spline boundary condition with the exact value of the second derivative at the boundaies. How does the error behaves now? What is the advantage of this method? [Hint: you have to compute and implement the second derivative of the function yourself. Evaluate it at the boundaries and substitute the appropriate values in A and RHS.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
